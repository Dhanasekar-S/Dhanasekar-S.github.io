<!DOCTYPE HTML>
<html lang="en">

<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    


    <title>Dhanasekar Sundararaman</title>
    
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link href="css/bootstrap.min.css" rel="stylesheet" media="screen">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

    <meta name="author" content="Dhanasekar Sundararaman">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="Description" content="Dhanasekar Sundararaman | Duke University | Microsoft | Google Research| Amazon Science">
    <meta name="keywords" content="Dhanasekar Sundararaman. Ph.D. student in ML/NLP at Duke University">

    <!-- <link rel="stylesheet" type="text/css" href="stylesheet.css"> -->
    <link rel="icon" type="image/Dharun_dp.jpg" href="images/Dharun_dp.jpg">
</head>
<body class="bg_colour">
<h1> Dhanasekar Sundararaman </h1>
    <table border=0 class="bg_colour" style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
            <td style="padding:0px">
                
                <!-- Name tab -->
                <table border=0 class="bg_colour" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    
                    
                    <tr style="padding:0px">
                        <td style="padding:2.5%;width:20%;max-width:20%">
                            <a href="images/Dharun_dp.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Dharun_dp.jpg" class="img-circle"></a>
                        </td>
                        <td style="padding:2.5%;width:60%;vertical-align:middle">
                            <p style="text-align:center">
                                <h1  style="text-align:center"><name></name></h1>
                            </p>
                            <p style="text-align:center">
                            &nbsp|&nbsp
                            <a href="#experience" onclick=Expand("experience")>Experience</a>
                            &nbsp|&nbsp
                            <a href="#publications" onclick=Expand("publications")>Publications</a>
                            &nbsp|&nbsp
                            <a href="mailto:ds448@duke.edu">Contact</a>
                            &nbsp|&nbsp
                            </p>
                        </td>
                        <td style="padding:2.5%;width:10%;max-width:10%">
                            <!-- <a href="images/Dharun_dp.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Dharun_dp.jpg" class="hoverZoomLink"></a> -->
                        </td>
                    </tr>
                    <!-- <tr>
                    <td colspan="2">
                        <p style="text-align:center">
                            &nbsp|&nbsp
                            <a href="#experience" onclick=Expand("experience")>Experience</a>
                            &nbsp|&nbsp
                            <a href="#publications" onclick=Expand("publications")>Publications</a>
                            &nbsp|&nbsp
                            <a href="#contact" onclick=Expand("contact")>Contact</a>
                            &nbsp|&nbsp
                        </p>
                    </td>
                    </tr> -->
                </tbody></table>


                <!-- About section, quick links -->

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr style="padding:0px">
                        <td style="padding:2.5%;width:63%;vertical-align:middle">
                            <p>I'm a Ph.D. candidate at Duke University working on ML/NLP. I'm fortunate to be advised by Prof. Lawrence Carin. My Ph.D. research interests focus on deep learning-based Natural Language Processing (NLP). In particular, Iâ€™m interested in incorporating classical NLP ideas on to deep learning frameworks to aid tasks like machine translation, natural language understanding, and reasoning.
                            </p>
                            
                            <p style="text-align:center">
                                &nbsp~&nbsp
                                <a href="mailto:ds448@duke.edu" target="_blank">Email</a> &nbsp|&nbsp
                                <a href="https://scholar.google.com/citations?user=O2jjlZIAAAAJ&hl=en&oi=ao" target="_blank" >Google Scholar</a> &nbsp|&nbsp
                                <a href="https://github.com/Dhanasekar-S" target="_blank" >Github</a> &nbsp|&nbsp
                                <a href="https://www.linkedin.com/in/dhanasekar95/" target="_blank">LinkedIn</a> &nbsp|&nbsp
                                &nbsp~&nbsp
                            </p>
                        </td>
                        <!-- <td style="padding:2.5%;width:30%;max-width:30%">
                            <a href="images/Dharun_dp.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Dharun_dp.jpg" class="hoverZoomLink"></a>
                        </td> -->
                    </tr>
                </tbody></table>
                <hr class="soft">

<!-- Experience -->

                <button style="border:0px transparent; background-color: transparent;outline:none;"type="button" class="collapsible" data-toggle="collapse" data-target="#content-experience" id="experience"><heading>Experience</heading></button>
                <div id="content-experience" class="collapse in">

                <table border=0 class="bg_colour" style="padding:20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                    <tr>
                        <td style="padding:10px;width:25%;vertical-align:middle">
                            <div class="one">
                                <img src='images/amazon.png' width="120">
                            </div>
                        </td>
                        <td style="padding:10px;width:75%;vertical-align:top">
                            <papertitle style="color:gray"><big>Research Scientist Intern</big> </papertitle> <papertitle ><big> | Amazon Science</big></papertitle>
                            <br>
                            May '20 - Aug '20
                            <br>
                            <br>
                            <p>NLP research internship focusing on topic modeling, summarization.</p>
                        </td>
                    </tr>
					
					<tr>
                        <td style="padding:10px;width:25%;vertical-align:middle">
                            <div class="one">
                                <img src='images/google.png' width="120">
                            </div>
                        </td>
                        <td style="padding:10px;width:75%;vertical-align:top">
                            <papertitle style="color:gray"><big>Research Intern</big> </papertitle> <papertitle ><big> | Google Research</big></papertitle>
                            <br>
                            Sep '20 - Dec '20
                            <br>
                            <br>
                            <p>Research intern at Google research. Worked on task weighting for multi-task learning for NLP.</p>
                        </td>
                    </tr>
					
					<tr>
                        <td style="padding:10px;width:25%;vertical-align:middle">
                            <div class="one">
                                <img src='images/microsoft.webp' width="120">
                            </div>
                        </td>
                        <td style="padding:10px;width:75%;vertical-align:top">
                            <papertitle style="color:gray"><big>Researcher Intern</big> </papertitle> <papertitle ><big> | Microsoft</big></papertitle>
                            <br>
                            June '21 - Aug '21
                            <br>
                            <br>
                            <p>Worked on Domain generalization, Semantic parsing.</p>
                        </td>
                    </tr>
					
					<tr>
                        <td style="padding:10px;width:25%;vertical-align:middle">
                            <div class="one">
                                <img src='images/amazon.png' width="120">
                            </div>
                        </td>
                        <td style="padding:10px;width:75%;vertical-align:top">
                            <papertitle style="color:gray"><big>Applied Scientist Intern</big> </papertitle> <papertitle ><big> | Amazon Science</big></papertitle>
                            <br>
                            Oct '21 - Dec '21
                            <br>
                            <br>
                            <p>NLP Applied Scientist Intern</p>
                        </td>
                    </tr>

                </tbody></table>
                </div>
                <hr class="soft">


<!-- publications -->

                <button style="border:0px transparent; background-color: transparent;outline:none;"type="button" class="collapsible" data-toggle="collapse" data-target="#content-publications" id="publications"><heading>Publications</heading></button>
                <div id="content-publications" class="collapse in">

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


					<tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <div class="one">
                                <img src='images/bias.jpg' width="120">
                            </div>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:top">
                            <papertitle><big>Exploring Gender Bias in Retrieval Models</papertitle></big>
                            <br>
                            <p><em>Arxiv Preprint <a href="https://arxiv.org/pdf/2208.01755.pdf" target="_blank">[preprint]</a></em></p>
                            <p>Biases in culture, gender, ethnicity, etc. have existed for decades and have affected many areas of human social interaction. These biases have been shown to impact machine learning (ML) models, and for natural language processing (NLP), this can have severe consequences for downstream tasks. Mitigating gender bias in information retrieval (IR) is important to avoid propagating stereotypes. In this work, we employ a dataset consisting of two components: (1) relevance of a document to a query and (2) "gender" of a document, in which pronouns are replaced by male, female, and neutral conjugations. We definitively show that pre-trained models for IR do not perform well in zero-shot retrieval tasks when full fine-tuning of a large pre-trained BERT encoder is performed and that lightweight fine-tuning performed with adapter networks improves zero-shot retrieval performance almost by 20% over baseline. We also illustrate that pre-trained models have gender biases that result in retrieved articles tending to be more often male than female. We overcome this by introducing a debiasing technique that penalizes the model when it prefers males over females, resulting in an effective model that retrieves articles in a balanced fashion across genders.</p>
                        </td>
                    </tr>
					
                    <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <div class="one">
                                <img src='images/nuer.jpg' width="120">
                            </div>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:top">
                            <papertitle><big>Improving Downstream Task Performance by Treating Numbers as Entities</papertitle></big>
                            <br>
                            <p><em>International Conference on Information and Knowledge Management '22. <a href="https://arxiv.org/pdf/2205.03559.pdf" target="_blank">[preprint]</a></em></p>
                            <p>Numbers are essential components of text, like any other word tokens, from which natural language processing (NLP) models are built and deployed. Though numbers are typically not accounted for distinctly in most NLP tasks, there is still an underlying amount of numeracy already exhibited by NLP models. For instance, in named entity recognition (NER), numbers are not treated as an entity with distinct tags. In this work, we attempt to tap the potential of state-of-the-art language models and transfer their ability to boost performance in related downstream tasks dealing with numbers. Our proposed classification of numbers into entities helps NLP models perform well on several tasks, including a handcrafted Fill-In-The-Blank (FITB) task and on question answering, using joint embeddings, outperforming the BERT and RoBERTa baseline classification.</p>
                        </td>
                    </tr>

                    <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <div class="one">
                                <img src='images/ur.png' width="120">
                            </div>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:top">
                            <papertitle><big>Learning Task Sampling Policy for Multitask Learning</papertitle></big>
                            <br>
                            <p>
                            <em>The 2021 Conference on Empirical Methods in Natural Language Processing. <a href="https://aclanthology.org/2021.findings-emnlp.375.pdf" target="_blank">[Paper]</a> 
                            </em>
                            </p>
                            <p>It has been shown that training multi-task models with auxiliary tasks can improve the target task quality through cross-task transfer. However, the importance of each auxiliary task to the primary task is likely not known a priori. While the importance weights of auxiliary tasks can be manually tuned, it becomes practically infeasible with the number of tasks scaling up. To address this, we propose a search method that automatically assigns importance weights. We formulate it as a reinforcement learning problem and learn a task sampling schedule based on the evaluation accuracy of the multi-task model. Our empirical evaluation on XNLI and GLUE shows that our method outperforms uniform sampling and the corresponding single-task baseline.</p>
                        </td>
                    </tr>
					
					<tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <div class="one">
                                <img src='images/featuresum.png' width="120">
                            </div>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:top">
                            <papertitle><big>Syntactic Knowledge-Infused Transformer and BERT Models</papertitle></big>
                            <br>
                            <p>
                            <em>International Conference on Information and Knowledge Management. <a href="http://ceur-ws.org/Vol-3052/short21.pdf" target="_blank">[Paper]</a> 
                            </em>
                            </p>
                            <p>Attention-based deep learning models have demonstrated significant improvement over traditional algorithms in several NLP tasks. The Transformer, for instance, is an illustrative example that generates abstract representations of tokens that are input to an encoder based on their relationships to all tokens in a sequence. While recent studies have shown that such models are capable of learning syntactic features purely by seeing examples, we hypothesize that explicitly feeding this information to deep learning models can significantly enhance their performance in many cases. Leveraging syntactic information like part of speech (POS) may be particularly beneficial in limited-training-data settings for complex models such as the Transformer. In this paper, we verify this hypothesis by infusing syntactic knowledge into the Transformer. We find that this syntax-infused Transformer achieves an improvement of 0.7 BLEU when trained on the full WMTâ€™14 English to German translation dataset and a maximum improvement of 1.99 BLEU points when trained on a fraction of the dataset. In addition, we find that the incorporation of syntax into BERT fine-tuning outperforms BERTBASE on all downstream tasks from the GLUE benchmark, including an improvement of 0.8% on CoLA.</p>
                        </td>
                    </tr>
					
					<tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <div class="one">
                                <img src='images/lexical.jpg' width="120">
                            </div>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:top">
                            <papertitle><big>How do lexical semantics affect translation? An empirical study</papertitle></big>
                            <br>
                            <p>
                            <em>Arxiv Preprint. <a href="https://arxiv.org/pdf/2201.00075.pdf" target="_blank">[Preprint]</a> 
                            </em>
                            </p>
                            <p>Neural machine translation (NMT) systems aim to map text from one language into another. While there are a wide variety of applications of NMT, one of the most important is translation of natural language. A distinguishing factor of natural language is that words are typically ordered according to the rules of the grammar of a given language. Although many advances have been made in developing NMT systems for translating natural language, little research has been done on understanding how the word ordering of and lexical similarity between the source and target language affect translation performance. Here, we investigate these relationships on a variety of low-resource language pairs from the OpenSubtitles2016 database, where the source language is English, and find that the more similar the target language is to English, the greater the translation performance. In addition, we study the impact of providing NMT models with part of speech of words (POS) in the English sequence and find that, for Transformer-based models, the more dissimilar the target language is from English, the greater the benefit provided by POS..</p>
                        </td>
                    </tr>
					
					<tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <div class="one">
                                <img src='images/vecproj2d.png' width="120">
                            </div>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:top">
                            <papertitle><big>Methods for Numeracy-Preserving Word Embeddings</papertitle></big>
                            <br>
                            <p>
                            <em>The 2020 Conference on Empirical Methods in Natural Language Processing. <a href="https://aclanthology.org/2020.emnlp-main.384.pdf" target="_blank">[Paper]</a> 
                            </em>
                            </p>
                            <p>Word embedding models are typically able to capture the semantics of words via the distributional hypothesis, but fail to capture the numerical properties of numbers that appear in the text. This leads to problems with numerical reasoning involving tasks such as question answering. We propose a new methodology to assign and learn embeddings for numbers. Our approach creates Deterministic, Independent-of-Corpus Embeddings (the model is referred to as DICE) for numbers, such that their cosine similarity reflects the actual distance on the number line. DICE outperforms a wide range of pre-trained word embedding models across multiple examples of two tasks:(i) evaluating the ability to capture numeration and magnitude; and (ii) to perform list maximum, decoding, and addition. We further explore the utility of these embeddings in downstream tasks, by initializing numbers with our approach for the task of magnitude prediction. We also introduce a regularization approach to learn model-based embeddings of numbers in a contextual setting.</p>
                        </td>
                    </tr>
					
					<tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <div class="one">
                                <img src='images/bemb.jpg' width="120">
                            </div>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:top">
                            <papertitle><big>Learning compressed sentence representations for on-device text processing</papertitle></big>
                            <br>
                            <p>
                            <em>The 2019 Conference on Association for Computational Linguistics. <a href="https://aclanthology.org/P19-1011.pdf" target="_blank">[Paper]</a> 
                            </em>
                            </p>
                            <p>Vector representations of sentences, trained on massive text corpora, are widely used as generic sentence embeddings across a variety of NLP problems. The learned representations are generally assumed to be continuous and real-valued, giving rise to a large memory footprint and slow retrieval speed, which hinders their applicability to low-resource (memory and computation) platforms, such as mobile devices. In this paper, we propose four different strategies to transform continuous and generic sentence embeddings into a binarized form, while preserving their rich semantic information. The introduced methods are evaluated across a wide range of downstream tasks, where the binarized sentence embeddings are demonstrated to degrade performance by only about 2% relative to their continuous counterparts, while reducing the storage requirement by over 98%. Moreover, with the learned binary representations, the semantic relatedness of two sentences can be evaluated by simply calculating their Hamming distance, which is more computational efficient compared with the inner product operation between continuous embeddings. Detailed analysis and case study further validate the effectiveness of proposed methods.</p>
                        </td>
                    </tr>
					
					<tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <div class="one">
                                <img src='images/twigraph.jpg' width="120">
                            </div>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:top">
                            <papertitle><big>Twigraph: Discovering and Visualizing Influential Words Between Twitter Profiles</papertitle></big>
                            <br>
                            <p>
                            <em>The 2017 International Conference on Social Informatics. <a href="https://arxiv.org/ftp/arxiv/papers/1706/1706.05361.pdf" target="_blank">[Paper]</a> 
                            </em>
                            </p>
                            <p>The social media craze is on an ever increasing spree, and people are
connected with each other like never before, but these vast connections are visually unexplored. We propose a methodology Twigraph to explore the connections
between persons using their Twitter profiles. First, we propose a hybrid approach of recommending social media profiles, articles, and advertisements to a user.
The profiles are recommended based on the similarity score between the user profile, and profile under evaluation. The similarity between a set of profiles is
investigated by finding the top influential words thus causing a high similarity through an Influence Term Metric for each word. Then, we group profiles of various domains such as politics, sports, and entertainment based on the similarity score through a novel clustering algorithm. The connectivity between profiles is envisaged using word graphs that help in finding the words that connect a set of profiles and the profiles that are connected to a word. Finally, we analyze the top influential words over a set of profiles through clustering by finding the similarity of that profiles enabling to break down a Twitter profile with a lot of followers to fine level word connections using word graphs. The proposed method was implemented on datasets comprising 1.1 M Tweets obtained from Twitter. Experimental results show that the resultant influential words were highly representative of the relationship between two profiles or a set of profiles.</p>
                        </td>
                    </tr>

                    

                    

                    

                    

                   

                    

                    
                    
                    
                </tbody></table>
                </div>
                <hr class="soft">

            </td>
        </tr>
        <tr>
            <td>
                <p>This template is a modification to Jon Barron's <a href="https://jonbarron.info/" target="_blank">website</a></p>
            </td>
        </tr>
        <tr>
            <td>
                <p></p>
            </td>
        </tr>
    </table>

</body>

</html>
